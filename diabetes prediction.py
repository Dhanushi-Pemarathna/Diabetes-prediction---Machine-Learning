# -*- coding: utf-8 -*-
"""Diabetes .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R5rBru5Vx9rTHUT2cOvZaaZqxDCvuGi9

Our dataset is a classification. Becase it is going to represent wether a purson has diabetes or not. So it is a classification problem.

Let's start!

###1.Setting up
"""

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/MyDrive/ML_Assignment")

"""###2.Reading the dataset"""

import pandas as pd

diabetes = pd.read_csv("diabetes_prediction_dataset_2 .csv")
diabetes.head()

diabetes.info()

"""###3.Explore the data"""

import seaborn as sns
import matplotlib.pyplot as plt

"""###3.1 Understaing the distribution of the features"""

features = diabetes.columns.drop('diabetes').tolist()

"""In our data set diabetes is the dependand variable. It means it is the 'Y' part."""

fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12,8))
axs = axs.flatten()

for index, feature in enumerate(features):
  sns.boxplot(data=diabetes, x=feature, hue='diabetes', ax=axs[index])

# Set the 6th subplot as invisible
axs[8].set_visible(False)
plt.show()

"""It create 8 subplots for selected feature from the dataset (diabetes).

###3.2.Check whether there are any highly correlated features
"""

corr_matrix = diabetes.corr()

fig, ax = plt.subplots(figsize=(10,6))
sns.heatmap(corr_matrix, annot=True, cmap = 'coolwarm', ax=ax)
plt.show()

"""### 4.Data Preprocessing

"""

print(diabetes.isnull().sum())

"""check wether null values have or not. It does not have null values. So we don't want to remove null values.

"""

print(diabetes.duplicated().sum())

"""check whether the duplicate values have or not(Row wise). it has 934 duplicates values. so it means we have to remove those values."""

diabetes = diabetes.drop_duplicates()

diabetes.duplicated().sum()

diabetes = diabetes.dropna()

data = diabetes.copy(deep=True)

data['gender'].value_counts()

"""we have to set categorical data into numerical data."""

gender = pd.get_dummies(data['gender'], drop_first=True)
data = pd.concat([data, gender], axis=1)
data = data.drop(['gender'], axis=1)

data.head()

diabetes['smoking_history'].value_counts()

"""dummy variable creation"""

smoking_history = pd.get_dummies(data['smoking_history'], drop_first=True)
data = pd.concat([data, smoking_history], axis=1)
data = data.drop(['smoking_history'], axis=1)

data.head()

data.info()

"""### 5.Train Test Split"""

from sklearn.model_selection import train_test_split

features_updated = data.columns.drop(['diabetes', 'blood_glucose_level']).tolist()

"""Removed the dipendent variable(y) and one hightly corerelated independent variable checking with the heatmap."""

print(features_updated)

X = data[features_updated]
y = data['diabetes']

# Checking for class imbalance
y.value_counts(normalize=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=123)

y_test.value_counts(normalize=True)

"""### 6.Model Development

#### 6.1.Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import LabelEncoder, StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Normalize features in machine learning training and testing data for consistent scale."""

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)

y_pred_log_reg

log_reg.feature_names_in_

log_reg.coef_

lr_base_coef_df = pd.DataFrame({'feature':log_reg.feature_names_in_ , 'lr_coef':log_reg.coef_[0]})
lr_base_coef_df

from sklearn.model_selection import  StratifiedKFold

stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

precision = precision_score(y_test, y_pred_log_reg)
recall = recall_score(y_test, y_pred_log_reg)
f1 = f1_score(y_test, y_pred_log_reg)
accuracy = accuracy_score(y_test, y_pred_log_reg)

"""This code calculates precision, recall, F1 score and accuracy for a logistic regression model's prediction on the test set."""

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Accuracy:", accuracy)

"""##### 6.1.2. Visualizations for classification metrices / reports"""

from yellowbrick.classifier import confusion_matrix
from yellowbrick.classifier import classification_report
from yellowbrick.classifier import roc_auc
from yellowbrick.classifier import class_prediction_error
from yellowbrick.classifier import discrimination_threshold
from yellowbrick.classifier import precision_recall_curve

conf_matrix_log_reg_base = confusion_matrix(log_reg, X_test, y_test, cmap="Oranges")

class_prediction_error(log_reg, X_test, y_test)

class_report_log_reg_base = classification_report(log_reg, X_test, y_test)

roc_log_reg_base = roc_auc(log_reg, X_test, y_test, binary=True)

prec_recall_log_reg = precision_recall_curve(log_reg, X_test, y_test)

disc_thresh_log_reg = discrimination_threshold(log_reg, X_test, y_test)

"""#### 6.2.Support Vector Machines"""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from yellowbrick.classifier import ConfusionMatrix, ClassPredictionError, ClassificationReport, ROCAUC, PrecisionRecallCurve, DiscriminationThreshold

X_train.head()

scaler = StandardScaler()

X_train_sc = X_train.copy(deep=True)
X_test_sc = X_test.copy(deep=True)

X_train_sc[X_train_sc.columns] = scaler.fit_transform(X_train_sc[X_train_sc.columns])
X_test_sc[X_test_sc.columns] = scaler.fit_transform(X_test_sc[X_test_sc.columns])

X_train_sc.head()

X_test_sc.head()

svc = SVC(kernel='linear')
svc.fit(X_train_sc, y_train)
y_pred_svc = svc.predict(X_test_scaled)

precision = precision_score(y_test, y_pred_svc)
recall = recall_score(y_test, y_pred_svc)
f1 = f1_score(y_test, y_pred_svc)
accuracy = accuracy_score(y_test, y_pred_svc)

"""This code calculates precision, recall, F1 score and accuracy for a Support vector machine predictions on the test set."""

print("Precision:",precision)
print("Recall:",recall)
print("F1-score:",f1)
print("Accuracy:",accuracy)

"""##### 6.2.1. Visualizations for classification metrices / reports"""

conf_matrix_svc = confusion_matrix(svc, X_test_sc, y_test, cmap="Oranges")
conf_matrix_svc.fit(X_train_scaled, y_train)
conf_matrix_svc.score(X_test_scaled, y_test)
conf_matrix_svc.show()

class_error_svm = ClassPredictionError(svc, classes=svc.classes_)
class_error_svm.score(X_test_scaled, y_test)
class_error_svm.show()

class_report_svm = ClassificationReport(svc, classes=svc.classes_, support=True, cmap="oranges")
class_report_svm.fit(X_train_scaled, y_train)
class_report_svm.score(X_test_scaled, y_test)
class_report_svm.show()

roc_svm = ROCAUC(svc, classes=svc.classes_, binary=True)
roc_svm.fit(X_train_scaled, y_train)
roc_svm.score(X_test_scaled, y_test)
roc_svm.show()

prec_recall_svm = PrecisionRecallCurve(svc, classes=svc.classes_)
prec_recall_svm.fit(X_train_scaled, y_train)
prec_recall_svm.score(X_test_scaled, y_test)
prec_recall_svm.show()

disc_thresh_svm = DiscriminationThreshold(svc)
disc_thresh_svm.fit(X_train_scaled, y_train)
disc_thresh_svm.score(X_test_scaled, y_test)
disc_thresh_svm.show()

"""Create a discrimination threshold plot

##### 6.2.2.Support Vector Machine Related Visualizations
"""

from sklearn.decomposition import PCA
from sklearn.inspection import DecisionBoundaryDisplay
import numpy as np

pca_2d = PCA(n_components=2)
pc_df = pd.DataFrame(pca_2d.fit_transform(X_train_sc))
pc_df.columns = ["PC1", "PC2"]

pc_df.head()

# Plotting settings
fig, ax = plt.subplots(figsize=(4, 3))
x_min, x_max, y_min, y_max = -3, 3, -3, 3
ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

# Plot samples by color and add legend
scatter = ax.scatter(pc_df.values[:, 0], pc_df.values[:, 1], s=2, c=y_train.values, cmap = 'viridis', alpha=0.4,label=y_train.values)
ax.legend(*scatter.legend_elements(), title="Classes", bbox_to_anchor=(1.05, 1.0), loc='upper left')
ax.set_title("Data in two-dimensional feature space")
_ = plt.show()

def plot_training_data_with_decision_boundary(kernel):
    # Train the SVC
    clf = SVC(kernel=kernel).fit(pc_df.values, y_train)

    # Settings for plotting
    _, ax = plt.subplots(figsize=(4, 3))
    x_min, x_max, y_min, y_max = -4, 4, -4, 4
    ax.set(xlim=(x_min, x_max), ylim=(y_min, y_max))

    # Plot decision boundary and margins
    common_params = {"estimator": clf, "X": pc_df.values, "ax": ax}
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="predict",
        plot_method="pcolormesh",
        alpha=0.3,
    )
    DecisionBoundaryDisplay.from_estimator(
        **common_params,
        response_method="decision_function",
        plot_method="contour",
        levels=[-1, 0, 1],
        colors=["k", "k", "k"],
        linestyles=["--", "-", "--"],
    )

    # Plot bigger circles around samples that serve as support vectors
    ax.scatter(
        clf.support_vectors_[:, 0],
        clf.support_vectors_[:, 1],
        s=5,
        facecolors="none",
        edgecolors="k",
    )
    # Plot samples by color and add legend
    ax.scatter(pc_df.values[:,0], pc_df.values[:, 1], c=y_train.values, s=2, alpha=0.4,  cmap = 'viridis')
    ax.legend(*scatter.legend_elements(), title="Classes", bbox_to_anchor=(1.05, 1.0), loc='upper left')
    ax.set_title(f" Decision boundaries of {kernel} kernel in SVC")

    _ = plt.show()

plot_training_data_with_decision_boundary("linear")

plot_training_data_with_decision_boundary("rbf")

"""#### 6.3.Decision Trees"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score

clf_dt = DecisionTreeClassifier(random_state=55)

clf_dt.fit(X_train, y_train)

y_pred_dt = clf_dt.predict(X_test)

accuracy_score(y_test, y_pred_dt)

accuracy_score(y_train, clf_dt.predict(X_train))

precision = precision_score(y_test, y_pred_dt)
recall = recall_score(y_test,y_pred_dt)
f1 = f1_score(y_test,y_pred_dt)
accuracy = accuracy_score(y_test,y_pred_dt)

"""This code calculates precision, recall, F1 score and accuracy for a decision tree predictions on the test set."""

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Accuracy:", accuracy)

"""##### 6.3.1. Visualizations for decision tree / reports"""

confusion_matrix(clf_dt, X_test, y_test, cmap="Oranges")

confusion_matrix(clf_dt, X_train, y_train, cmap="oranges")

class_report_log_reg_base = classification_report(clf_dt, X_test, y_test)

plt.figure(figsize=(200,50))
plot_tree(clf_dt,
          filled=True,
          rounded=True,
          class_names=True,
          feature_names=X_train.columns);

path = clf_dt.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path['ccp_alphas']

ccp_alpahs = ccp_alphas[:-1]

clf_dts = []

for ccp_alpha in ccp_alphas:
  clf_dt_ = DecisionTreeClassifier(random_state=55, ccp_alpha=ccp_alpha)
  clf_dt_.fit(X_train, y_train)
  clf_dts.append(clf_dt_)

train_scores = [clf_dt_.score(X_train, y_train) for clf_dt_ in clf_dts]
test_scores = [clf_dt_.score(X_test, y_test) for clf_dt_ in clf_dts]

"""plotting the train scores and test scores"""

fig, ax = plt.subplots(figsize=(10,8))
ax.set_xlabel("ccp alpha values")
ax.set_ylabel("Accuracy Score")
ax.set_title("Accuracy vs. ccp_alpha for training and testing")

ax.plot(ccp_alphas, train_scores, marker='o', label="Train", drawstyle='steps-post')
ax.plot(ccp_alphas, test_scores, marker='x', label="Test", drawstyle='steps-post')
ax.legend()
plt.show()

# for this case, 0.3 was selected by eye balling the above plot
ccp_alpha_best = 0.15

clf_dt_pruned = DecisionTreeClassifier(random_state=55, ccp_alpha=ccp_alpha_best)
clf_dt_pruned.fit(X_train, y_train)
y_pred_res_pruned = clf_dt_pruned.predict(X_test)

confusion_matrix(clf_dt_pruned, X_train, y_train, cmap="oranges")

confusion_matrix(clf_dt_pruned, X_test, y_test, cmap="oranges")

plt.figure(figsize=(200,50))
plot_tree(clf_dt_pruned,
          filled=True,
          rounded=True,
          class_names=True,
          feature_names=X_train.columns);

"""###6.4. Random forest"""

from sklearn.ensemble import RandomForestClassifier
from yellowbrick.model_selection import FeatureImportances

clf_rf= RandomForestClassifier()
clf_rf.fit(X_train, y_train)
y_pred_clf_rf = clf_rf.predict(X_test)

precision = precision_score(y_test, y_pred_clf_rf)
recall = recall_score(y_test, y_pred_clf_rf)
f1 = f1_score(y_test, y_pred_clf_rf)
accuracy = accuracy_score(y_test, y_pred_clf_rf)

"""This code calculates precision, recall, F1 score and accuracy for a random forest predictions on the test set."""

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Accuracy:", accuracy)

confusion_matrix(clf_rf, X_train, y_train, cmap="Oranges")

class_report_log_reg_base = classification_report(clf_rf, X_test, y_test)

roc_log_reg_base = roc_auc(clf_rf, X_test, y_test, binary=True)

viz = FeatureImportances(clf_rf)
viz.fit(X, y)
viz.show

"""### 7.Model comparison

"""

from sklearn.metrics import RocCurveDisplay

ax = plt.gca()

LogReg = RocCurveDisplay.from_estimator(log_reg, X_test, y_test,
                                        ax=ax, name="Logistic Regression")
SVC = RocCurveDisplay.from_estimator(svc, X_test_sc, y_test,
                                        ax=ax, name="Support Vector Classifier")

DTPruned = RocCurveDisplay.from_estimator(clf_dt_pruned, X_test, y_test,
                                        ax=ax, name="Decison Tree Pruned")

RandForest = RocCurveDisplay.from_estimator(clf_rf, X_test, y_test,
                                        ax=ax, name="Random Forest")


plt.show()

"""### What is the suitable algorithm

**Logistic Regression**

* Precision: 0.8882539682539683
* Recall: 0.5497053045186641
* F1-score: 0.6791262135922331
* Accuracy: 0.9318977951782402



**Support Vector Machine**

* Precision: 0.9890848026868178
* Recall: 0.4628683693516699
* F1-score: 0.6306209850107066
* Accuracy: 0.9289099526066351


**Decision Tree**
* Precision: 0.623121387283237
* Recall: 0.6353634577603143
* F1-score: 0.6291828793774319
* Accuracy: 0.9018133113538018


**Random Forest**
* Precision: 0.8303811057434246
* Recall: 0.6078585461689587
* F1-score: 0.7019056261343013
* Accuracy: 0.9323099113950134



The Random Forest model appears to strike a reasonable balance between precision, recall, and overall accuracy. It has a good overall accuracy, a decent F1-score, and a relatively high precision, indicating a low rate of false positives. This random forest model shows a good balance between precision and recall while maintaining a high overall accuracy.

### **Challenges and solutions**

* when doing data preprocessing I faced some problems. when chane categorical data into numerical data I faced Some problems. because smoking_history has 6 type of data. But after I used same method for that also. After I could complete it.


* when doing the model selection part, in that also I had to face some problems. Because it hard to understand what is the best and suitable one. That why I used other model(random forest) as a 4th model. So in that case I consider about the reasonable balance between precision recall, f1 and accuracy. So random forest model shows a good balance between precision and recall while maintaining a high overall accuracy. so I selected it.

* When visualizing among models I could not understand every diagrams. so it also was a problem for me. As a solution I watched some youtube videos to understand those diagrams.
"""